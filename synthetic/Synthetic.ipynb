{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b4e7279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8dba048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 80]) torch.Size([300, 20]) torch.Size([300])\n"
     ]
    }
   ],
   "source": [
    "# data generation\n",
    "N = 100 # data per domain\n",
    "D = 100 # data dimension\n",
    "batch_size = 128\n",
    "ratio = 0.8 # ratio of the number of invariant features \n",
    "sigma_inv = 1\n",
    "sigma_spu = 0.1\n",
    "sigma_factor = [2, 100]\n",
    "\n",
    "\n",
    "Z_1 = torch.normal(0, 1, (N, int(round(D*ratio)))).repeat(len(sigma_factor)+1,1)\n",
    "theta_1 = torch.normal(0, sigma_inv, (int(round(D*ratio)), 1))\n",
    "Y = ((Z_1 @ theta_1).squeeze() > 0).to(torch.float32) # Y is a binary vector with 0 and 1\n",
    "Z_2 = [torch.normal(1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1).repeat(1,int(round(D*(1-ratio)))), sigma_spu).to(torch.float32)]\n",
    "# Z_2 = [torch.normal((2*Y[:N]-1).unsqueeze(1).repeat(1,int(round(D*(1-ratio)))), sigma_spu).to(torch.float32)]\n",
    "for d, factor in enumerate(sigma_factor):\n",
    "    Z_2.append((Z_2[0]-1/(D*(1-ratio))*(2*Y[:N]-1).unsqueeze(1))*factor+2/D*(2*Y[:N]-1).unsqueeze(1))\n",
    "Z_2 = torch.cat(Z_2)\n",
    "print(Z_1.shape, Z_2.shape, Y.shape)\n",
    "Z = torch.cat((Z_1,Z_2),1)\n",
    "while True:\n",
    "    matrix = torch.randn(D, D)  # Create a DxD matrix with random values between 0 and 1\n",
    "    if torch.linalg.matrix_rank(matrix) == D:\n",
    "        Q, R = torch.linalg.qr(matrix)\n",
    "        break\n",
    "X = Z @ Q\n",
    "\n",
    "X_train, X_test = X[:N*(len(sigma_factor))], X[N*(len(sigma_factor)):]\n",
    "Y_train, Y_test = Y[:N*(len(sigma_factor))], Y[N*(len(sigma_factor)):]\n",
    "Z_train, Z_test = Z[:N*(len(sigma_factor))], Z[N*(len(sigma_factor)):]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class PairedDomainDataset(Dataset):\n",
    "    def __init__(self, X, Y, Z, domain_labels):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.domain_labels = domain_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X) // 2  # Half the size since we're pairing\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get a pair of indices from the same domain\n",
    "        idx1 = idx\n",
    "        idx2 = idx + len(self.X) // 2\n",
    "        return self.X[idx1], self.Y[idx1], self.X[idx2], self.Y[idx2]\n",
    "\n",
    "# Generate domain labels (assuming the first half is one domain, the second half is the other)\n",
    "domain_labels = np.concatenate([np.zeros(N * len(sigma_factor)), np.ones(N)])\n",
    "\n",
    "# Create the paired dataset\n",
    "paired_dataset = PairedDomainDataset(X_train, Y_train, Z_train, domain_labels)\n",
    "\n",
    "\n",
    "paired_loader = DataLoader(paired_dataset, batch_size=batch_size, shuffle=True) \n",
    "\n",
    "loader = DataLoader(TensorDataset(X_train, Y_train), batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f218b798",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LogisticRegression\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(X_train, Y_train)\n",
    "clf.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4d95e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Oracle (ERM using invariant feature only)\n",
    "clf = LogisticRegression(random_state=0, fit_intercept=False).fit(Z_train[:,:int(round(D*ratio))], Y_train)\n",
    "clf.score(Z_test[:,:int(round(D*ratio))], Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5dcbd1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple NN model.\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class LR(nn.Module):\n",
    "    def __init__(self, D_in):\n",
    "        super(LR, self).__init__()\n",
    "        self.linear_1 = nn.Linear(D_in, D_in)\n",
    "        self.linear_2 = nn.Linear(D_in, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.classifier(self.featurizer(x))\n",
    "\n",
    "    def featurizer(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        return x\n",
    "    \n",
    "    def classifier(self, x):\n",
    "        x = self.linear_2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x.squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc51f0cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.6200000047683716\n",
      "1.0 0.699999988079071\n",
      "1.0 0.699999988079071\n",
      "1.0 0.699999988079071\n",
      "1.0 0.6499999761581421\n",
      "1.0 0.6700000166893005\n",
      "1.0 0.6800000071525574\n",
      "1.0 0.6700000166893005\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.6800000071525574\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.6700000166893005\n",
      "1.0 0.6700000166893005\n",
      "1.0 0.6600000262260437\n",
      "1.0 0.6800000071525574\n",
      "1.0 0.6399999856948853\n",
      "1.0 0.6700000166893005\n",
      "1.0 0.6499999761581421\n",
      "1.0 0.7300000190734863\n",
      "---\n",
      "mean: 0.6794999837875366std: 0.02874113619327545\n"
     ]
    }
   ],
   "source": [
    "# ERM using our NN model\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "sum_in = []\n",
    "sum_out = []\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X, Y) in enumerate(loader):\n",
    "            y = model(X)\n",
    "            loss = criterion(y, Y)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in.append(float(((y_train>0.5)==Y_train).sum()/len(y_train)))\n",
    "    sum_out.append(float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "print(\"---\")\n",
    "print(\"mean: \" + str(torch.tensor(sum_out).mean().item()) + \"std: \" + str(torch.tensor(sum_out).std().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbc60fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "1.0 1.0\n",
      "---\n",
      "1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# CFM\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "sum_in = 0\n",
    "sum_out = 0\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    lmda = 1000\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X_1, Y_1, X_2, Y_2) in enumerate(paired_loader):\n",
    "            z_1 = model.featurizer(X_1)\n",
    "            z_2 = model.featurizer(X_2)\n",
    "            y_1 = model.classifier(z_1)\n",
    "            y_2 = model.classifier(z_2)\n",
    "            loss_1 = criterion(torch.cat((y_1,y_2)), torch.cat((Y_1,Y_2)))\n",
    "            # print(z_1.shape,z_2.shape)\n",
    "            loss_2 = (torch.norm(z_1-z_2, p=2) / len(z_1)) ** 2\n",
    "            # print(loss_1,loss_2)\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in +=  float(((y_train>0.5)==Y_train).sum()/len(y_train))\n",
    "    sum_out += float(((y_test>0.5)==Y_test).sum()/len(y_test))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "    \n",
    "print(\"---\")\n",
    "print(sum_in/repeats, sum_out/repeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ae8ab4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_pair: 1; mean: 0.6220000386238098; std: 0.038879022002220154\n",
      "num_of_pair: 2; mean: 0.6720000505447388; std: 0.054733797907829285\n",
      "num_of_pair: 3; mean: 0.6540001034736633; std: 0.05103146657347679\n",
      "num_of_pair: 4; mean: 0.6609999537467957; std: 0.04216508939862251\n",
      "num_of_pair: 5; mean: 0.6664999723434448; std: 0.04858795925974846\n",
      "num_of_pair: 6; mean: 0.6639999747276306; std: 0.05761944130063057\n",
      "num_of_pair: 7; mean: 0.6940000057220459; std: 0.06777672469615936\n",
      "num_of_pair: 8; mean: 0.6995000243186951; std: 0.05679835006594658\n",
      "num_of_pair: 9; mean: 0.6864999532699585; std: 0.055276818573474884\n",
      "num_of_pair: 10; mean: 0.7024999260902405; std: 0.04666509851813316\n",
      "num_of_pair: 11; mean: 0.7464999556541443; std: 0.040036171674728394\n",
      "num_of_pair: 12; mean: 0.7430000305175781; std: 0.06489668786525726\n",
      "num_of_pair: 13; mean: 0.7489999532699585; std: 0.060253847390413284\n",
      "num_of_pair: 14; mean: 0.75; std: 0.06341176480054855\n",
      "num_of_pair: 15; mean: 0.8075000047683716; std: 0.06850854307413101\n",
      "num_of_pair: 16; mean: 0.8170000910758972; std: 0.05272570997476578\n",
      "num_of_pair: 17; mean: 0.8555000424385071; std: 0.08947066217660904\n",
      "num_of_pair: 18; mean: 0.902999997138977; std: 0.0692136287689209\n",
      "num_of_pair: 19; mean: 0.9530000686645508; std: 0.03826500475406647\n",
      "num_of_pair: 20; mean: 1.0; std: 0.0\n",
      "num_of_pair: 21; mean: 1.0; std: 0.0\n",
      "num_of_pair: 22; mean: 1.0; std: 0.0\n",
      "num_of_pair: 23; mean: 1.0; std: 0.0\n",
      "num_of_pair: 24; mean: 1.0; std: 0.0\n",
      "num_of_pair: 25; mean: 1.0; std: 0.0\n",
      "num_of_pair: 26; mean: 1.0; std: 0.0\n",
      "num_of_pair: 27; mean: 1.0; std: 0.0\n",
      "num_of_pair: 28; mean: 1.0; std: 0.0\n",
      "num_of_pair: 29; mean: 1.0; std: 0.0\n",
      "num_of_pair: 30; mean: 1.0; std: 0.0\n",
      "num_of_pair: 31; mean: 1.0; std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Few shot CF Pair\n",
    "epoch = 100\n",
    "\n",
    "repeats = 20\n",
    "\n",
    "\n",
    "shots = np.arange(1,100)\n",
    "for shot in shots:\n",
    "    sum_in = []\n",
    "    sum_out = []\n",
    "\n",
    "    for repeat in range(repeats):\n",
    "        model = LR(D)\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        lmda = 2000\n",
    "        model.train()\n",
    "        for i in range(epoch):\n",
    "            for j, (X, Y) in enumerate(loader):\n",
    "                X= torch.cat((X_train[0:shot], X, X_train[N:N+shot]))\n",
    "                Y = torch.cat((Y_train[0:shot], Y, Y_train[N:N+shot]))\n",
    "                z = model.featurizer(X)\n",
    "                y = model.classifier(z)\n",
    "                loss_1 = criterion(y, Y)\n",
    "                loss_2 = (torch.norm(z[:shot]-z[-shot:], p=2) / shot)\n",
    "                loss = loss_1 + lmda * loss_2\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        model.eval()\n",
    "        y_test = model(X_test)\n",
    "        y_train = model(X_train)\n",
    "        sum_in.append(float(((y_train>0.5)==Y_train).sum()/len(y_train)))\n",
    "        sum_out.append(float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "        # print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "    print(\"num_of_pair: \" + str(shot) + \"; mean: \" + str(torch.tensor(sum_out).mean().item()) + \"; std: \" + str(torch.tensor(sum_out).std().item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bf0d33cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 0.7300000190734863\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7699999809265137\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.699999988079071\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.7099999785423279\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7599999904632568\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.6600000262260437\n",
      "1.0 0.7400000095367432\n",
      "1.0 0.7200000286102295\n",
      "1.0 0.7300000190734863\n",
      "1.0 0.7699999809265137\n",
      "---\n",
      "1.0 0.7310000061988831\n"
     ]
    }
   ],
   "source": [
    "# IRM\n",
    "\n",
    "repeats = 20\n",
    "sum_in = 0\n",
    "sum_out = 0\n",
    "\n",
    "\n",
    "import torch.autograd as autograd\n",
    "\n",
    "scale = torch.tensor(1.).requires_grad_()\n",
    "\n",
    "\n",
    "def irm_penalty(loss_0, loss_1):\n",
    "    grad_0 = autograd.grad(loss_0.mean(), [scale], create_graph=True)[0]\n",
    "    grad_1 = autograd.grad(loss_1.mean(), [scale], create_graph=True)[0]\n",
    "    result = torch.sum(grad_0 * grad_1)\n",
    "    del grad_0, grad_1\n",
    "    return result\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    model = LR(D)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "    penalty_weight = 1\n",
    "\n",
    "    model.train()\n",
    "    for i in range(epoch):\n",
    "        for j, (X_1, Y_1, X_2, Y_2) in enumerate(paired_loader):\n",
    "            z_1 = model.featurizer(X_1)\n",
    "            z_2 = model.featurizer(X_2)\n",
    "            y_1 = model.classifier(z_1)\n",
    "            y_2 = model.classifier(z_2)\n",
    "            loss_1 = criterion(y_1*scale, Y_1)\n",
    "            loss_2 = criterion(y_2*scale, Y_2)\n",
    "            loss_3 = irm_penalty(loss_1, loss_2)\n",
    "            # print(loss_1,loss_2)\n",
    "            loss = loss_1 + lmda * loss_2\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    model.eval()\n",
    "    y_test = model(X_test)\n",
    "    y_train = model(X_train)\n",
    "    sum_in +=  float(((y_train>0.5)==Y_train).sum()/len(y_train))\n",
    "    sum_out += float(((y_test>0.5)==Y_test).sum()/len(y_test))\n",
    "    print(float(((y_train>0.5)==Y_train).sum()/len(y_train)), float(((y_test>0.5)==Y_test).sum()/len(y_test)))\n",
    "\n",
    "print(\"---\")\n",
    "print(sum_in/repeats, sum_out/repeats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
